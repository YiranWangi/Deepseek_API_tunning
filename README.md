# DeepSeek API Prompt Tuning for Medical QA

## Project Overview

This project investigates how different prompt designs affect the quality of responses generated by the **DeepSeek LLM API** in the domain of **diabetes-related medical question answering (QA)**. By crafting domain-specific prompts and systematically comparing outputs, we aim to develop an **effective and safe prompting strategy** for medical chatbot applications.


## Objectives

- Leverage the **DeepSeek Chat Completion API** to generate medical QA responses
- Compare multiple **prompting strategies**: basic, expert-style, and patient-friendly
- Evaluate outputs on dimensions such as **factuality**, **fluency**, and **harmlessness**
- Generate structured evaluation logs and human-readable reports for comparison


## Key Components

| File | Description |
|------|-------------|
| `deepseek tunning.py` | Main script: prompt generation, DeepSeek API call, output logging |
| `diabetes_qa_pairs.json` | Source QA pairs (diabetes-related) used as test inputs |
| `prompt_comparison_*.json` | Structured evaluation output (per prompt style and response) |
| `prompt_report_*.txt` | Human-readable report comparing different prompting styles |


## Prompting Strategies

We implement and compare three prompt formats:
- **Basic**: direct question as-is
- **Expert Mode**: answered in a clinical tone, with medical terms and diagnostics
- **Patient Mode**: simplified language, gentle tone, practical suggestions

Each question is run through all prompt styles, and the model’s responses are recorded.


## How to Run

1. Set your DeepSeek API key inside the script.
2. Run the script via your Python environment:
   ```bash
   python deepseek tunning.py
   ```
3. Example Output
   ```bash
    尿病问答提示词优化实验
    ==================================================
    正在运行对比实验...
    已完成问题: What are the early symptoms of diabetes mellitus?
   ...
    结果已保存到 prompt_comparison_20250609_xxxxxx.json
    报告已生成: prompt_report_20250609_xxxxxx.txt
   ```
